= QUICKSTART

=== Simple Transliteration Model

We implement the following simple transliteration model:

* a -> b, b -> c, c -> d, d -> e, ..., z -> a

The steps below are going to allow us to implement a transliteration model:

1. *Design Diagrams*
2. *Diagram Based Code*
2. *Build Transliteration Data*
3. *Neural Network Model*
4. *Run Ruby Code*


=== 1. Design Diagrams

We have designed the following diagram with lucidchart:
 https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/resources/FullDemo.pdf[diagram]


It represents a simple strategy that we broke down into 3 steps:

* Transliteration
* Preprocessor
* Mapping

A strategy can be thought as a structure through which data "flows"
and gets processed by the various operations represented by the nodes (~functional programming).

=== 2. Transliteration Code Generation

===== Install Julia and dependencies
Link to https://julialang.org/downloads/[install julia].
source,sh]
----
# going to learn-graph repo.
> cd learn-graph
# install julia deps
> julia packageInstall.jl
----

===== Generate code from diagram

Diagram designs on lucidchart can be exported as csv.
The csv can in turn be transformed into code
(.dat format with julia serialization).


[source,sh]
----
# build code from diagram
> julia train.jl --path-lucidchart-csv resources/FullDemo.csv --brain-entry transliteration --path-model resources/FullDemo.dat
----

The code should output a list of warning messages:

┌ Warning: ("unimplemented Node:: Id", 6, " Name: ", "map all letters utilising table and @ to a")

This means that the logic needs to be coded.
This is done under */learn-graph/src/Rules.jl*.

So we do:
[source,sh]
----
# copy code for nodes
> cp resources/RulesSamples/Rules.jl src/Rules.jl
# and rebuild
> julia train.jl --path-lucidchart-csv resources/FullDemo.csv --brain-entry transliteration --path-model resources/FullDemo.dat
----
This time the code should be successfully built.


===== Create code from dir

Alternatively, code can be generated from multiple .csv files
as the ones in *learn-graph/resources/modelDir/*.

[source,sh]
----
> cd learn-graph
> ls resources/modelDir/*csv
'resources/modelDir/Demo Mappings.csv'
'resources/modelDir/Demo Transliteration.csv'
'resources/modelDir/Demo Preprocessor.csv'
> julia train.jl --dir-path-lucidchart-csv resources/modelDir/ --brain-entry transliteration --path-model resources/DirDemo.dat
----


===== Run Code, Tests and transliteration

====== Run DEBUG mode
We find useful to run the built code with a full
verbose mode:
[source,bash]
----
> julia runDBGCode.jl --path-model resources/FullDemo.dat --text "abcd efgh"
Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing)
[ Info: ("brain name ::> ", "transliteration")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing))
[ Info: ("brain name ::> ", "preprocessor")
[ Info: ("node::> ", "normalize the text!")
Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing)
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing))
[ Info: ("node::> ", "tokenize the text!")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "preprocessor", "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "process each word with mapping")
[ Info: ("brain name ::> ", "mapping")
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => nothing, "wrd" => "abcd", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "abcd", "res" => "bcde", "state" => "no"))
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "abcd", "res" => "bcde", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("brain name ::> ", "mapping")
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => nothing, "wrd" => "efgh", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "efgh", "res" => "defg", "state" => "no"))
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "efgh", "res" => "defg", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "preprocessor", "l_transl_wrds" => ["bcde", "defg"], "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "bind transliterated words together")
[ Info: ("data::> ", Dict{String, Any}("txt" => "bcde defg", "brain" => "preprocessor", "l_transl_wrds" => ["bcde", "defg"], "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "done, terminate")
bcde defg
----
This allows to track the states and debug the codes.

====== Run Tests
Runs tests in /data/test_benchmark.csv, shows bugs and write them to csv:
[source,bash]
----
> julia run.jl --path-model resources/FullDemo.dat --file-name test
words accuracy: 1.0
error summary in: data/test_debug.csv
----
Errors were written in data/test_debug.csv.

====== Run Transliteration
[source,bash]
----
# run transliteration
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test_train.csv
# run transliteration into file
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test_train.csv --file-name-out testout.txt
----

=== 4. Neural networks

===== Install Python
Python 3.8 is supported and tested.

===== Generate Transliteration Data
After transliteration has been generated, as in Run Transliteration,
training data is created.
[source,bash]
----
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test_train.csv --file-name-out data/test_train.csv
100.0%┣████████████████████████████████████████┫ 830/830 [00:00<00:00, 2.2kit/s]

real	0m40.863s
user	0m40.212s
sys	0m0.572s
----

===== Train Neural Networks

The example below should have converged against the test
examples after ~20 epochs. 

For practical implementation we found useful to train the nnets with lots of passes
and data, lowering also the training gradually. Scores should be near 100% for an accurate implementation
in ruby.

The demo is using a transformer version and strategy encoding words at the word level. 
The code in /python-nnets-torch is the one for the encoding at char level and using a slightly simpler setup.

[source,bash]
----
> cd python-nnets-demo-word-level
# install dependencies
> pip install -r requirements.txt
# train nnets
> python script_train_transformer_on_transliteration.py
100%|███████████████████████████████████████████| 34/34 [00:14<00:00,  2.36it/s]
Epoch: 1,            Train loss: 7.030,            Val loss: 6.627,       git      Epoch time = 14.431s
...
...
...
{'accuracy': 1.0}
save model: data/model_basic_epoch_25.pt
----

===== Export Neural Networks to Onnx
As next step, on can export the transformer to ONNX format.
This occurs after its decomposition into submodels.
[source,bash]
----
# export to onnx
> python script_transformers_decomposed_to_onnx.py
test:
source:  a
target:   b
Export token src embbedding
Export token tgt embbedding
Export Positional Encoding
Export Generator
Export Encoder
Export Decoder
Write Vocab Transform
----

=== 5. Run ruby Code
Finally, one can run the *ruby code*.
In principle, necessary variable are specified in *config/params.yml*.
[source,bash]
----
> cd ../lib
# transliterate string
> ruby script_transliteration.rb --text "ab"
bc
> ruby script_transliteration.rb --text "z"
a
ruby script_transliteration.rb --text "bc"
c d (code did not recognised bc but b and c)
> ruby script_transliteration.rb --text  "sibi temperat ignis"
tjcj ufnqfsbu jhojt
> time ruby script_transliteration.rb --text "zkldndmdwvft"
"" (the code has never seen this word and can not decompose it into two words)
# transliterate file
echo ab > test_train.csv
echo z >> test_train.csv
> ruby script_transliteration.rb --text_filename test_train.csv
bc
a
----
