0. When we face the affix ست, always a suffix, if there's any of the sounds /A, i, u/ right before it, it must be transliterated /st/, else, /ast/. I added a row for /st/.

1. If a word can be found in the database, there's no need to break it down (stem for nouns and lemmatize for verbs) UNLESS it's recognized as a verb while the word in entries is not marked as a verb. Note that this is to improve the accuracy of the system. So, if a verb exists in entries but we skip that transliteration because it's marked as a noun, and we end up not recognizing parts of the word (e.g. not all affixes are recognized), we can go back to the skipped transliteration and use it as a last resort.

2. For nouns that aren't found in the database, we need to use the stemmer function to get roots of the nouns. Then, much like with verbs, we should look and see which affixes were omitted from the word, find them in the Affixes table, and put them in proper positions.

3. When breaking down verbs to their roots using lemmatizer function, we get two outputs. If only one of them exists in the verb we have broken down, we use that one. But if both of them exist in the verb we have broken down, if we have ب, بی as a prefix, we use the second output of the lemmatizer. Else, we use the first output of the lemmatizer. Then we look to see which affixes were omitted from the verb when we broke it down, and search for them in the Affixes table and add their transliteration to the root of the verb in proper positions, i.e. prefixes get added to the beginning of the word and suffixes, to its end.

4. For collisions, if the PoS tagging doesn't help, please use the transliteration with the higher frequency.

5. For verbs that contain underscores after being lemmatized, we need to look for the parts separately. So, for خواهند كرد, you get the output خواهند_كرد from the lemmatizer. Now, if we look up خواهند and كرد in the dataset separately, we find the latter one and use it directly (it's a collision, but since we know it's a verb, we use the proper one which is /kard/), and for خواهند, we can lemmatize it again. The output will be خواست#خواه. Since we have خواه in the verb, we look it up in the dataset and use the transliteration /xAh/. Then we see that ند has been omitted from the verb, so we look it up in the Affixes table, and add that (/and/) to the end of the verb root so the whole word will be /xAhand/ which is the concatenation of the two in proper order. So, in the end, the transliteration of خواهند كرد will be /xAhand kard/ Please note that the space is not added. It existed in the middle of خواهند كرد, and the underscore just reminded us of the fact that they are separate words.

6. The library works only with standard Farsi letters {> stemmer.stem('گزارشی')'گزارش' > stemmer.stem('گزارشي') 'گزارشي')} and I already converted entries, affixes, etc. to the standard letters. So, from now on, replace the letters ي and ك in inputs to ی and ک.

7. The affix ی, always a suffix, never a prefix, is used with verbs and nouns. For verbs, it has only one transliteration. If the word before it, is not a verb, which will usually be a noun (not always), if the transliteration of the word before it ends in /a,e,o,A,u/, the last transliteration of the noun kind, /ye/, should be used, else, the first transliteration should be used. I added PoS tags to the last two ones since they're for verbs.

8. Wherever there's the letter ۀ as in the word همۀ the transliteration of the word containing it should end in /eye/ based on the database transliteration system. That letter can also be written down as هٔ as in the word همهٔ. So it may consist of one or two characters. Note that the word containing this letter already ends in /e/ so we only need to add the /ye/ part to the end of it.

9. The affix ات is a collision. Note that it's always a suffix, never a prefix. When faced, if it's attached to the word with no space or semispace, the first transliteration should be chosen. If there's a space or semispace before it, then choose the second  transliteration.

10. The affix ان is a collision. Two of its instances are marked for nouns. It's always a suffix, never a prefix and to choose between the two, when it's attached to a noun or adjective, if the noun/adjective ends in /i/, choose the last transliteration, else, choose the first transliteration.

11. The affix ش, always a suffix, is also a collision. If it's attached to a noun, choose the first transliteration, if it's anything else, choose the second transliteration.

12. The affix م, always a suffix, is a collision. According to the PoS tagging in affixes, the different transliteration should be used with numbers, and if the word before it isn't a number, the main transliteration should be used, UNLESS the word before it is چند. For this word, the different transliteration should be used.

13. The affix مان, always a suffix, is a collision. If the word or segment right before it ends in /a,e,o,A,i,u/ choose the second transliteration, else, use the first one.

14. The affix می is a collision. If it's a prefix, we know it's part of a verb and we use the second transliteration. If it's a suffix, we use the first transliteration even if the word before it isn't a number.

15. If we lemmatize a verb, but none of the two results are part of the verb we have lemmatized, it's possible the root of the verb has an آ letter that's been mentioned in our verb in ا form. If the verb roots have آ, change it to ا and check rule 3. Make sure to undo the change to the verb root to be able to find it in the database.

16. The affix ون, always a suffix, is a collision. To choose between the two, if the word before it, ends in /i/, choose the second transliteration, else, choose the first transliteration.

17. The affix ید, always a suffix, is a collision. If it's not attached to a verb, (so it doesn't need to be a noun. It can be an adjective, etc.) there's only one transliteration (the first one) But if it's attached to a verb, if the root of the verb before it has been the first output of the lemmatize function, always use the first transliteration, /id/, but if it has been the second output of the lemmatize function, {if it ends in /e, A, u/, the second transliteration should be used, /yad/, else, the first transliteration, /id/, should be used} I added PoS tags to this affix in the proper column.

18. The affix یم, always a suffix, is a collision. If it's not attached to a verb, (so it doesn't need to be a noun. It can be an adjective, etc.) return /im/ unless the segment before it ends in /a, e, o, A, i, u/. In that case, return /yam/. But if it's attached to a verb, if the root of the verb before it has been the first output of the lemmatize function, always use the first transliteration, /im/, but if it has been the second output of the lemmatize function, {if it ends in /e, A, u/, the second transliteration should be used, /yam/, else, the first transliteration, /im/, should be used} I added PoS tags to this affix in the proper column, and added a row for /yam/ for verbs.

19. Semispace (ZWNJ): Wherever we see the character u200c, we should break the sides of it and work on them separately. The point, though, is that most of the time, it breaks affixes  from roots, but not always. So, when we encounter one, we would wanna check for the parts separately. Most of the time when we see this character, there's going to be می or نمی before it, or ها after it. One point to consider here, is that می and نمی, which are verb prefixes, are exactly these values, but ها which is a noun suffix, is the beginning of a string, so it may not be exactly ها. but it starts with ها. If these aren't found next to the u200c charcater, we should look for both sides of it in entries.

20. If after breaking down a word into its root, the remaining parts aren't found in affixes, we should look for matching substrings. For example, the noun کتاب‌هایتان is stemmed into کتاب which can be found is entries. The remaining part, هایتان, however, is not among affixes. So, what we should do in these cases, is to look for the longest substring of the remaining part that can be found in affixes starting where the string starts. So, while we can go with the affix ه which is the starting letter of the remaining string, we won't, because ها which is a longer substring, can be found in affixes, too. Now why won't we go with یتان that is a substring of the remaining string and is actually even longer? Because it doesn't start where the remaining string starts. So, we go with ها and find it in affixes and add the transliteration to the root of the word. Then, we do the same with the remaining string, i.e. the string that is not transliterated yet. So, this is a recursive algorithm. In the case of this example, we find یتان in the affixes. Done! Note that this goes for verbs and lemmatization, too. So, we don't stop until the whole word is transliterated.

21. The affix ن is a collision. It's always part of a verb. When it's a prefix, use the first transliteration and when it's a suffix, use the last transliteration, the one I added.

22. When affixes بی and نی, both verb prefixes, are used, we need to omit the ' symbol that comes after them.

23. If the root of the verb is رو, transliterated /rav/, and it doesn't have any suffixes, we should change its transliteration from /rav/ to /ro/.

24. With all the other rules, there are still many words that won't be recognized due to a PoS tagging error or the word being new. Let's take a look at an example: The whole کرده اند is a verb, but PoS tagger has recognized کرده as an adjective and اند as a verb. Fortunately, کرده exists in the entries, so the code has successfully recognized it, but اند has remained untransliterated since it's an affix for verbs, not a verb root, thus it's in affixes, not entries! So, if a word isn't recognized after everything we've done according to all the rules, we might wanna consider checking the affixes as it will solve issues like the one in this example. Now if the word is not even there, we can break it down according to rule 20, i.e. we must look for the word's substrings, but this time in entries. Now, this can cause a lot of bugs, so we only do it if we've had no luck recognizing a word with all the other rules. So, instead of just writing output in Farsi, we want to try and come up with an estimated transliteration! Let's look into the word فیسبوک as an example. This is the Farsi written form of the word "Facebook". We don't find this word in entries. So, normally, the code would output the word as is, i.e. in Farsi. However, with the described method, first, the substring فیس will be found in entries, then بو, then ک can be found in affixes. So, the output would be /fisbuak/ To make this rule more clear, let's see another example. The library fails to stem دستمان correctly. It simply won't recognize the word دست which is Farsi for hand. It stems it into دس which doesn't exist in entries. recursive: {So, we take the whole word, دستمان, and try to find the longest substring of it we can find in entries. We find دست! Bingo! Then we can find مان and we have the whole /dastemAn/ Note that after we found دست in entries, we need to look up the remaining part in affixes and see if we have the whole remaining part in there. If not, we can go back to entries and look up the remaining string there}
Note 1: When the longest substring found in a string is in the middle of it, two substrings remain on its sides. Our next step is to look for each substring in affixes, and if it's not there, each substring must be transliterated separately. So, it may be checked using PoS tagger and get stemmed/lemmatized.
Note 2: At the point where we have only single letters left to transliterate, whether the last remaining string is only a letter or no bigger string can be found, we should use the single letters with T as their PoS. I added them to stand for transliteration.