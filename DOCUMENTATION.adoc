= QUICKSTART

=== Simple Transliteration Model

We implement the following simple transliteration model:

* a -> b, b -> c, c -> d, d -> e, ..., z -> a

The steps below are going to allow us to implement a transliteration model:

1. *Design Diagrams*
2. *Diagram Based Code*
  * install julia and dependencies
  * generate code
  * run DEBUG code
  * run tests
2. *Build Transliteration Data*
3. *Neural Network Model*
  * train model
  * export model to onnx
4. *Run Ruby Code*


=== 1. Design Diagrams

We have designed the following diagram with lucidchart:
 https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/resources/FullDemo.pdf[diagram]


It represents a simple strategy that we broke down into 3 steps:

* Transliteration, bundling the steps below
* Preprocessor: clean up chars, lower case, ...
* Mapping: applying the above transformation on characters

A strategy can be thought as a structure through which data "flows"
and gets processed by the various operations represented by the nodes (~functional programming).

In the design, the following conventions are currently supported:

1. *Entries*
    * represent: logic flows starts
    * "Curly Brace Note"
2. *Nodes*
    * represent: computational nodes specifying uniquely some operations
    * "Process", "Decision", "Terminator"
3. *Connections*
  * represent: logical steps
  * "unlabelled directed arrows" "labelled directed arrows"
   (in arbitrary number)

Entries are activated in the following ways:

1. specifying main entry when building code
2. calling entry via node: e.g. "Preprocessor"
3. or calling for a recursion or loop: e.g. "process each word with mapping"

So in the diagram, the computational flow is jumping between subdiagrams.

=== 2. Transliteration Code Generation

===== Install Julia and dependencies
Link to https://julialang.org/downloads/[install julia].
source,sh]
----
# going to learn-graph repo.
> cd learn-graph
# install julia deps
> julia packageInstall.jl
----

===== Generate code from diagram

Diagram designs on lucidchart can be exported as csv.
The csv can in turn be transformed into code
(.dat format with julia serialization).


[source,sh]
----
# build code from diagram
> julia train.jl --path-lucidchart-csv resources/FullDemo.csv --brain-entry transliteration --path-model resources/FullDemo.dat
----

The code should output a list of warning messages:

â”Œ Warning: ("unimplemented Node:: Id", 6, " Name: ", "map all letters utilising table and @ to a")

This means that the logic needs to be coded.
This is done under */learn-graph/src/Rules.jl*.

So we do:
[source,sh]
----
# copy code for nodes
> cp resources/RulesSamples/Rules.jl src/Rules.jl
# and rebuild
> julia train.jl --path-lucidchart-csv resources/FullDemo.csv --brain-entry transliteration --path-model resources/FullDemo.dat
----
This time the code should be successfully built.

===== Details on Code Snippets Implementation

Code building is based on the following ideas and tools:

[source,ruby]
----
# computation state
dataSTATE = Dict{String, Any}(
            "state" => nothing,
            "brain" => nothing);

# Dictionary with the commands
dicCODE = Dict{String, Functor}()
----

Next examples are basic node implementations:
[source,ruby]
----
# Node terminating computation
dicCODE["done, terminate"] =
    #===
        Basic form of functors:
            d: data
            e: dicBRAINS
            f: df_Nodes

        Inputs and Outputs are specified
        :in => "l_transliterated" # list of
        :out => "res" # field expected at end of (sub)sequence
    ===#
    Functor((d,e=nothing,f=nothing) ->
        begin
            d["res"] = d["txt"]
            d
        end, # identity
        Dict(:in => ["txt"],
             :out => ["res"]))

dicCODE["bind transliterated words together"] =
    #===
      Implimentation of simple node
    ===#
    Functor((d,e=nothing,f=nothing) ->
        begin
          # ["a", "cat"] -> "a cat"
          d["txt"] = join(d["l_transl_wrds"], " ");
          d
        end,
        Dict(:in => ["l_transl_wrds"],
             :out => ["txt"]))

----

Above, for more code stability, :in and :out fields necessary for the
computational flow to be performed most be specified.
"res" allow to terminate a (sub)flow returning a particular value rather than
the full computation state.

It can be useful to review how to call an other part of the
diagram and here also to loop over that process.

[source,ruby]
----
dicCODE["apply mappings on each word"] =
    Functor((d,e=nothing,f=nothing) ->
        begin
          d["l_transl_wrds"] =
            map(wrd ->
                begin
                    dd = copy(dataSTATE)
                    dd["wrd"] = wrd
                    interfaceName = "mapping"
                    node = e[interfaceName]
                    runAgent(node, e, f, dd)
                end,
                d["l_wrds"])
          d
        end,
        Dict(:in => ["l_wrds"],
             :out => ["l_transl_wrds"]))
----

More examples can be found under *learn-graph/resources/RulesSamples/*
and code and functions can be copied *learn-graph/src/Rules.jl*.


===== Create code from dir

Alternatively, code can be generated from multiple .csv files
as the ones in *learn-graph/resources/modelDir/*.
This approach allows for more  a more atomic approach, sub components
can be separated and tweaked.
[source,sh]
----
> cd learn-graph
> ls resources/modelDir/*csv
'resources/modelDir/Demo Mappings.csv'
'resources/modelDir/Demo Transliteration.csv'
'resources/modelDir/Demo Preprocessor.csv'
> julia train.jl --dir-path-lucidchart-csv resources/modelDir/ --brain-entry transliteration --path-model resources/DirDemo.dat
----


===== Run Python, external code and others

====== Python Snippets & Modules
[source,ruby]
----
using PyCall

py"""
latin_chars = 'abcdefghijklmnopqrstuvwxyz '

def do_whatever(txt):
    ...
    return whatever

d_dic = {'a': 'b', ...}
"""

# assets and code can be called in following fashions
py"""do_whatever"""("some text")

py"""d_dic"""['a']

# as do the modules be imported and used in the code (example hazm for farsi):
hazm = pyimport("hazm")
stemmer = hazm.Stemmer()
lemmatizer = hazm.Lemmatizer()
normalizer = hazm.Normalizer()
tagger = hazm.POSTagger(model=PATH_HAZM)
----


With the julia module https://www.juliapackages.com/p/pycall[pycall].

Alternatively, the python code can be put in another file, e.g. py_code.jl, written between
"""py ... """ and called as in our farsi code:
[source,ruby]
----
include("rel_path/py_code.jl")
----

====== External Programs

To run external program and bash commands and process their imputs,
one can proceed as follows:
[source,ruby]
----
# > ./a.out $word
read(`a.out $word`, String)
# > echo $wrd | sed s/z/@/g
read(pipeline(`echo $wrd`, `sed s/z/@/g`), String)
----

===== Run Code, Tests and transliteration

====== Run DEBUG mode
We find useful to run the built code with a full
verbose mode:
[source,bash]
----
> julia runDBGCode.jl --path-model resources/FullDemo.dat --text "abcd efgh"
Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing)
[ Info: ("brain name ::> ", "transliteration")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing))
[ Info: ("brain name ::> ", "preprocessor")
[ Info: ("node::> ", "normalize the text!")
Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing)
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing))
[ Info: ("node::> ", "tokenize the text!")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "preprocessor", "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "process each word with mapping")
[ Info: ("brain name ::> ", "mapping")
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => nothing, "wrd" => "abcd", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "abcd", "res" => "bcde", "state" => "no"))
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "abcd", "res" => "bcde", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("brain name ::> ", "mapping")
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => nothing, "wrd" => "efgh", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "efgh", "res" => "defg", "state" => "no"))
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "efgh", "res" => "defg", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "preprocessor", "l_transl_wrds" => ["bcde", "defg"], "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "bind transliterated words together")
[ Info: ("data::> ", Dict{String, Any}("txt" => "bcde defg", "brain" => "preprocessor", "l_transl_wrds" => ["bcde", "defg"], "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "done, terminate")
bcde defg
----
This allows to track the states and debug the codes.

====== Run Tests
Runs tests, shows bugs and write them to csv:
[source,bash]
----
> julia run.jl --path-model resources/FullDemo.dat --file-name test
words accuracy: 1.0
error summary in: data/test_debug.csv
----
Errors were written in data/test_debug.csv.

====== Run Transliteration
[source,bash]
----
# run transliteration
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test.txt
# run transliteration into file
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test.txt --file-name-out testout.txt
----

=== 4. Neural networks

===== Install Python
Python 3.6 is supported,
https://www.python.org/downloads/release/python-360/[link for installation],
alternatives are https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html[conda]
and https://docs.conda.io/projects/conda/en/latest/commands/create.html[conda create environment].

===== Generate Transliteration Data
Afert transliteration has been generated, as in Run Transliteration,
training data is created.
[source,bash]
----
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test.txt --file-name-out data/test_train.csv
100.0%â”£â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”« 830/830 [00:00<00:00, 2.2kit/s]

real	0m40.863s
user	0m40.212s
sys	0m0.572s
----

===== Train Neural Networks

The example below should have converged against the test
examples after ~15 epochs.
[source,bash]
----
> cd python-nnets-torch
# install dependencies
> pip install -r requirements.txt
# train nnets
> python script_train_transformer_on_transliteration.py
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:14<00:00,  2.36it/s]
Epoch: 1,            Train loss: 7.030,            Val loss: 6.627,       git      Epoch time = 14.431s
...
...
...
{'accuracy': 1.0}
save model: data/model_basic_epoch_20.pt
----

===== Export Neural Networks to Onnx
As next step, on can export the transformer to ONNX format.
This occurs after its decomposition into submodels.
[source,bash]
----
# export to onnx
> python script_transformers_decomposed_to_onnx.py
test:
source:  a
target:   b
Export token src embbedding
Export token tgt embbedding
Export Positional Encoding
Export Generator
Export Encoder
Export Decoder
Write Vocab Transform
----

=== 5. Run ruby Code
Finally, one can run the *ruby code*.
In principle, necessary variable are specified in *config/params.yml*.
[source,bash]
----
> cd lib
# transliterate string
> ruby script_transliteration.rb --text "ab"
bc
> ruby script_transliteration.rb --text "z"
a
ruby script_transliteration.rb --text "bc"
c d (code did not recognised bc but b and c)
> ruby script_transliteration.rb --text  "sibi temperat ignis"
tjcj ufnqfsbu jhojt
> time ruby script_transliteration.rb --text "zkldndmdwvft"
"" (the code has never seen this word and can not decompose it into two words)
# transliterate file
echo ab > test.txt
echo z >> test.txt
> ruby script_transliteration.rb --text_filename test.txt
bc
a
----
