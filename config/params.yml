transliteration:
  window: 6
  space: 4
  preprocessData: true

nnets:

  batch_size: 64
  block_size: 256 # maximum context length (sentence length) for predictions?
  max_iters: 5000 # maximum number of iterations for training
  eval_interval: 500 # evaluate every n iterations
  eval_iters: 200 # number of iterations to evaluate on
  learning_rate: 3e-4 # learning rate
  n_embd: 384 # embedding dimension
  n_head: 6 # number of attention heads
  n_layer: 6 # number of transformer blocks
  dropout: 0.2 # dropout rate

file_paths:
  file_name: 'gdrive/MyDrive/out.txt' # path to the comma separated text file
  n_cutoff: 1000000 # number of lines to read from the file (throwing away the rest)
  chars_file_path: <FULLPATH/chars.txt> # path to the file containing the list of characters
  torch_model_path: <FULLPATH/transformer_model.pt> # path to the torch model
  onnx_model_path: <FULLPATH/transformer_model.onnx> # path to the onnx model

  
