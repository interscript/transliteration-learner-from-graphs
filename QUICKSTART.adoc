= QUICKSTART

=== Simple Transliteration Model

We implement the following simple transliteration model:

* a -> b, b -> c, c -> d, d -> e, ..., z -> a

The steps below are going to allow us to implement:

1. *design diagram*
2. *diagram based code*
  * generate code
  * run tests
2. *build transliteration data*
3. *neural network model*
  * train model
  * export model to onnx
4. *run ruby code*

=== Design Diagram

We have designed the following diagram with lucidchart:
 https://github.com/interscript/transliteration-learner-from-graphs/blob/main/learn-graph/resources/Model1.0.png[diagram]

It represents a simple strategy that we broke down into 3 steps:

* Transliteration, bundling the steps below
* Preprocessor: clean up chars, lower case, ...
* Mapping: applying the above transformation on characters

A strategy can be thought as a structure through which data "flows"
and gets processed by the various operations represented by the nodes (~functional programming).

In the design, the following conventions are currently supported:

1. *Entries*
    * represent: logic flows starts
    * "Curly Brace Note"
2. *Nodes*
    * represent: computational nodes specifying uniquely some operations
    * "Process", "Decision", "Terminator"
3. *Connections*
  * represent: logical steps
  * "unlabelled directed arrows" "labelled directed arrows"
   (in arbitrary number)

Entries are activated in the following ways:

1. specifying main entry when building code
2. calling entry via node: e.g. "Preprocessor"
3. or calling for a recursion or loop: e.g. "process each word with mapping"

So in the diagram, the computational flow is jumping between subdiagrams.

=== Transliteration Code Generation

Diagram designs on lucidchart can be exported as csv.


[source,sh]
----
# going to learn-graph repo.
cd learn-graph
# install deps
julia packageInstall.jl
julia train.jl --path-lucidchart-csv resources/FullDemo.csv --brain-entry transliteration --path-model resources/FullDemo.dat
----

The code should output a list of warning messages:

┌ Warning: ("unimplemented Node:: Id", 6, " Name: ", "map all letters utilising table and @ to a")

This means that the logic needs to be coded.
This is done under */learn-graph/src/Rules.jl*.


===== Code Snippets Implementation

Code building is based on the following ideas and tools:

[source,ruby]
----
# computation state
dataSTATE = Dict{String, Any}(
            "state" => nothing,
            "brain" => nothing);

# Dictionary with the commands
dicCODE = Dict{String, Functor}()
----

Next examples are basic node implementations:
[source,ruby]
----
# Node terminating computation
dicCODE["done, terminate"] =
    #===
        Basic form of functors:
            d: data
            e: dicBRAINS
            f: df_Nodes

        Inputs and Outputs are specified
        :in => "l_transliterated" # list of
        :out => "res" # field expected at end of (sub)sequence
    ===#
    Functor((d,e=nothing,f=nothing) ->
        begin
            d["res"] = d["txt"]
            d
        end, # identity
        Dict(:in => ["txt"],
             :out => ["res"]))

dicCODE["bind transliterated words together"] =
    #===
      Implimentation of simple node
    ===#
    Functor((d,e=nothing,f=nothing) ->
        begin
          # ["a", "cat"] -> "a cat"
          d["txt"] = join(d["l_transl_wrds"], " ");
          d
        end,
        Dict(:in => ["l_transl_wrds"],
             :out => ["txt"]))

----

Above, for more code stability, :in and :out fields necessary for the
computational flow to be performed most be specified.
"res" allow to terminate a (sub)flow returning a particular value rather than
the full computation state.

It can be useful to review how to call an other part of the
diagram and here also to loop over that process.

[source,ruby]
----
dicCODE["apply mappings on each word"] =
    Functor((d,e=nothing,f=nothing) ->
        begin
          d["l_transl_wrds"] =
            map(wrd ->
                begin
                    dd = copy(dataSTATE)
                    dd["wrd"] = wrd
                    interfaceName = "mapping"
                    node = e[interfaceName]
                    runAgent(node, e, f, dd)
                end,
                d["l_wrds"])
          d
        end,
        Dict(:in => ["l_wrds"],
             :out => ["l_transl_wrds"]))
----

More examples can be found under *learn-graph/resources/RulesSamples/*
and code and functions can be copied *learn-graph/src/Rules.jl*.


===== Create code from dir

Alternatively, code can be generated from multiple .csv files
as the ones in *learn-graph/resources/modelDir/*.
This approach allows for more  a more atomic approach, sub components
can be separated and tweaked.
[source,sh]
----
cd learn-graph
julia train.jl --dir-path-lucidchart-csv resources/modelDir/ --brain-entry preprocessor --path-model resources/DirDemo.dat
----


===== Run Python, external code and others

====== Python Snippets & Modules
[source,ruby]
----
using PyCall

py"""
latin_chars = 'abcdefghijklmnopqrstuvwxyz '

def do_whatever(txt):
    ...
    return whatever

d_dic = {'a': 'b', ...}
"""

# assets and code can be called in following fashions
py"""do_whatever"""("some text")

py"""d_dic"""['a']

# as do the modules be imported and used in the code:
hazm = pyimport("hazm")
stemmer = hazm.Stemmer()
lemmatizer = hazm.Lemmatizer()
normalizer = hazm.Normalizer()
tagger = hazm.POSTagger(model=PATH_HAZM)
----


With the julia module https://www.juliapackages.com/p/pycall[pycall].

Alternatively, the python code can be put in another file, e.g. py_code.jl, written between
"""py ... """ and called as in our farsi code:
[source,ruby]
----
include("rel_path/py_code.jl")
----

====== External Programs

To run external program and bash commands and process their imputs,
one can proceed as follows:
[source,ruby]
----
# > ./a.out $word
read(`a.out $word`, String)
# > echo $wrd | sed s/z/@/g
read(pipeline(`echo $wrd`, `sed s/z/@/g`), String)
----

===== Run Tests and transliteration

====== Run DBG mode
We find useful to run the built code with a full
verbose mode:
[source,bash]
----
> julia runDBGCode.jl --path-model resources/FullDemo.dat --text "abcd efgh"
Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing)
[ Info: ("brain name ::> ", "transliteration")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing))
[ Info: ("brain name ::> ", "preprocessor")
[ Info: ("node::> ", "normalize the text!")
Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing)
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "transliteration", "state" => nothing))
[ Info: ("node::> ", "tokenize the text!")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "preprocessor", "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "process each word with mapping")
[ Info: ("brain name ::> ", "mapping")
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => nothing, "wrd" => "abcd", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "abcd", "res" => "bcde", "state" => "no"))
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "abcd", "res" => "bcde", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("brain name ::> ", "mapping")
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => nothing, "wrd" => "efgh", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "efgh", "res" => "defg", "state" => "no"))
[ Info: ("node::> ", "has word the char z?")
[ Info: ("response::> ", "no")
[ Info: ("data::> ", Dict{String, Any}("brain" => "mapping", "wrd" => "efgh", "res" => "defg", "state" => "no"))
[ Info: ("node::> ", "map all letters utilising table and @ to a")
[ Info: ("data::> ", Dict{String, Any}("txt" => "abcd efgh", "brain" => "preprocessor", "l_transl_wrds" => ["bcde", "defg"], "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "bind transliterated words together")
[ Info: ("data::> ", Dict{String, Any}("txt" => "bcde defg", "brain" => "preprocessor", "l_transl_wrds" => ["bcde", "defg"], "l_wrds" => SubString{String}["abcd", "efgh"], "state" => nothing))
[ Info: ("node::> ", "done, terminate")
bcde defg
----
This allows to track the states and debug the codes.

====== Run Transliteration
[source,bash]
----
# run transliteration
julia run.jl --path-model resources/FullDemo.dat --file-name data/test.txt
# run transliteration into file
julia run.jl --path-model resources/FullDemo.dat --file-name data/test.txt --file-name-out testout.txt
----

====== Run Tests
Runs tests, shows bugs and write them to csv:
[source,bash]
----
> julia run.jl --path-model resources/FullDemo.dat --file-name test
words accuracy: 1.0
error summary in: data/test_debug.csv
----
Errors were written in data/test_debug.csv.

=== Neural networks

===== Generate Transliteration Data
Afert transliteration has been generated, as in Run Transliteration,
training data is created.
[source,bash]
----
> julia run.jl --path-model resources/FullDemo.dat --file-name data/test.txt --file-name-out data/test_train.txt
100.0%┣████████████████████████████████████████┫ 830/830 [00:00<00:00, 2.2kit/s]

real	0m40.863s
user	0m40.212s
sys	0m0.572s
----
===== Train Neural Networks
[source,bash]
----
> python script_train_transformer_on_transliteration.py
100%|███████████████████████████████████████████| 34/34 [00:14<00:00,  2.36it/s]
Epoch: 1,            Train loss: 7.030,            Val loss: 6.627,       git      Epoch time = 14.431s
----
===== Export Neural Networks to Onnx

=== Run ruby Code
